cache_folder: llm/cache

embeddings:
  embeddings_path: llm/embeddings_obsidian
  embedding_model:
    type: sentence_transformer
    model_name: "intfloat/e5-large-v2"
  splade_config:
    n_batch: 2

 # Support for multi-chunking
  chunk_sizes:
    - 1024

  document_settings:
  - doc_path: /home/smnair/work/llms/citta
  # - doc_path: llm/docs
    exclude_paths:
      - /home/smnair/work/llms/citta/templates
      - /home/smnair/work/llms/unread
      - /home/smnair/work/llms/citta/journal
    scan_extensions: 
      - md
      - pdf
    additional_parser_settings:
      md: 
        skip_first: True
        merge_sections: False
        remove_images: True
        # find_metadata:
        #   description: "description:"
    passage_prefix: "passage: "
    label: "obsidian"

  # - doc_path: llm/pdf_docs2
  #   scan_extensions: 
  #     - pdf
  #   passage_prefix: "passage: "
  #   label: "books"


semantic_search:
  search_type: similarity # mmr
  max_k: 10

  replace_output_path:
    - substring_search: /home/smnair/work/llms/citta
      substring_replace: obsidian://open?vault=citta&file=

  append_suffix:
    append_template: "&heading={heading}"

  max_char_size: 4096
  query_prefix: "query: "
  multiquery:
    enabled: True
  reranker:
    model: "bge"

persist_response_db_path: responses_test.db


# llm:
#    type: openai
#    params:
#      prompt_template: |
#        Contex information is provided below. Given only the context and not prior knowledge, provide detailed answer to the question and references to the provided context. If answer isn't in the context, say you don't know.
        
#          ### Context:
#          ---------------------
#          {context}
#          ---------------------

#          ### Question: {question}
#      model_kwargs:
#        temperature: 0.0
#        model_name: gpt-3.5-turbo


llm:
 type: llamacpp
 params:
   model_path: llm/cache/airoboros-l2-13b-gpt4-1.4.1.Q4_K_M.gguf
   prompt_template: |
         ### Instruction:
         Use the following pieces of context to provide detailed answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.

         ### Context:
         ---------------
         {context}
         ---------------

         ### Question: {question}
         ### Response:
   model_init_params:
     n_ctx: 1512
     n_batch: 512
     n_gpu_layers: 25

   model_kwargs:
     max_tokens: 1024
     top_p: 0.1
     top_k: 40
     temperature: 0.2
     # mirostat_mode: 1




### An attempt to load 33B model on RTX 3060 with 10GB VRAM
# llm:
#   type: llamacpp
#   params:
#     model_path: llm/cache/airoboros-33b-ggml/airoboros-33b-gpt4-1.2.ggmlv3.q4_K_S.bin
#     prompt_template: |
#           ### Instruction:
#           Use the following pieces of context to answer the question at the end. If answer isn't in the context, say that you don't know, don't try to make up an answer.
#
#           ### Context:
#           ---------------
#           {context}
#           ---------------
#
#           ### Question: {question}
#           ### Response:
#     model_kwargs:
#       n_ctx: 1024
#       max_tokens: 512
#       temperature: 0.0
#       n_gpu_layers: 20
#       n_batch: 512

